---
title: "Final Info Sheet to Excel"
author: "Cameron Smith"
date: "12/12/2018"
output: html_document
---
```{r echo=FALSE, message=FALSE, prompt=FALSE}
library(infer)
library(resampledata)
library(tidyverse)
library(MASS)
library(dplyr)
#fullterm data is TXBirths where the age is 25-29 gestation is greater than 38 but less or equal to than 42 and no multiples.
fullterm <- TXBirths2004 %>% 
  filter(MothersAge == "25-29", Gestation >= 38, 
         Gestation <= 42, Multiple == "No")

## ------------------------------------------------------------------------
```

#Last Exam Q&A

```{r lastexam}
#1A: Probability that t with 5 degrees of freedom is greater than or equal to -0.5
ans1A <- 1 - pt(-0.5,5)
#OR
ans1A1 <- pt(-0.5, 5, lower = FALSE)

#1B: Probability that z <= 2
ans1B <- pnorm(2)

#1C: Chisq with 3 degrees of freedom >= 4
ans1C <- 1 - pchisq(4, 3)
#OR
ans1C1 <- pchisq(4, 3, lower = FALSE)

#1D: Probability that a t distribution with 25 degrees of freedom less than or equal to a is equal to .98, where a equals....
ans1D <- qt(.98, 25)

#1E: Probability that a is lessthan or equal to z which is less than or equal to 1 is equal to .5, where a is....
ans1E <- qnorm(pnorm(1) - .5)
ans1E1 <- pnorm(1) - pnorm(qnorm(pnorm(1) - 0.5))

```
1A: $P(t_5 \geq -0.5)$ == `r ans1A` = `r ans1A1`

1B: $P(z \leq 2)$ == `r ans1B`

1C: $P(x^2_3 \geq 4)$ == `r ans1C` = `r ans1C1`

1D: $P(t_25 \leq a) = .98$ find $a$ == `r ans1D`

1E: $P(a \leq z \leq 1) = 0.5$ find $a$ == `r ans1E` = `r ans1E1`


Question 3: Suppose you flip 100 biased coins, each with a probability of 0.70 of coming up heads.
```{r lastexam1}
ans3A <- rbinom(n = 100, size = 100, p = 0.7)
hist(ans3A)


```
3A: Define the random variable (X) and specify its distribution

>>X is the number of heads from flipping the coin 100 times of a bias coin.
>> X ~ Bin(n = 100, p = 0.70) == 
`r ans3A`

3B: Find the E(X) and V(X) exactly, and verify the reasonableness of your answer with a simulation using 10,000 flips.
```{r 3B}
# number of coins times the probability of getting heads
EX <- 100 * 0.7
# number of coins times the probability of getting heads times 1 minus the probability of getting heads (or the probability of getting tails)
VX <- 100 * 0.7 * (1-0.7)

#simulation of 10000 flips
draws <- rbinom(10000, 100, 0.7)
#approximately 70
mean(draws)
#approximately 21
var(draws)
```
E(X) = `r EX`
V(X) = `r VX`

3C: What is the probability 80 or more of the coins are heads? Provide an exact answer, and use a simulation of 10000 trials to verify the reasonableness of your exact answer.
```{r}
#exact answer
ans3C <- sum(dbinom(80:100, 100, 0.7))
#OR
ans3C1 <- 1 - pbinom(80 - 1, 100, .7)

#simulation
ans3Csim <- mean(rbinom(10000, 100, .7) >= 80)
```
exact: `r ans3C` == `r ans3C1`

simulation: `r ans3Csim`


3D: Suppose you obeserve 80 heads after flipping the biased coin 100 times. Use the central limit theorem to approximate the $P(X \geq 80)$
$X \sim N(70, \sqrt{21})$
```{r}
ans3D <- pnorm(80, 70, sqrt(21), lower = FALSE)
#OR
ans3D1 <- 1 - pnorm(80, 70, sqrt(21))
```
`r ans3D` == `r ans3D1`

4: If $X \sim Bin(n = 20, p = 0.5), Y \sim Bin(n = 30, p = 0.2)$ and X and Y are independent, what is the E(4X - 2Y) and V(4X - 2Y)? Provide an exact answer and use simulation with 10,000 trials to verify the reasonableness of your exact answer.

$E(4X - 2Y) = E(4X) - E(2Y) = 4(20* .5) - 2(30 * .2) = 4 * 10 - 2 * 6 = $ 

`r (4*10 - 2*6)`

$V(4X - 2Y) = 16V(X) + 4V(Y) = 16(20 * .5 * (1-.5)) + 4(30 * .2 * (1-.2)) = $ 

`r (16 * (20 * .5 * (1-.5)) + 4 * (30 * .2 * (1-.2)))`

```{r}
#Question 4
X <- rbinom(10000, 20, .5)
Y <- rbinom(10000, 30, .2)
mean(4*X - 2*Y)
var(4*X - 2*Y)

```

7: Use the data in Spruce to create a linear model where Di.change is regressed on Ht.change. Store your model in an object named model. Find and interpret the $R^2$ value for the model.  Create and interpret a 90% confidence interval for the mean Di.change for trees with a Ht.change of 40cm.
```{r question7}
model <- lm(Spruce$Di.change ~ Spruce$Ht.change)
summary(model)$r.squared
#This means the r squared value of the variability in Di.change can be accounted for by changes in Ht.change

CI <- predict(model, newdata = data.frame(Ht.change = 40, interval = "conf", level = 0.9))
CI
#We are 90% confident that the mean Di.change for trees with a Ht.change of 40cm falls in the interval above.
```
---

```{r}



## Pre test Code 

#fullterm data is TXBirths where the age is 25-29 gestation is greater than 38 but less or equal to than 42 and no multiples.
fullterm <- TXBirths2004 %>% 
  filter(MothersAge == "25-29", Gestation >= 38, 
         Gestation <= 42, Multiple == "No")

## ------------------------------------------------------------------------
set.seed(49)
library(infer)
#bootstrapping weight 1000 times from full term and calculating the mean.
bdis <- fullterm %>% 
specify(response = Weight) %>% 
generate(reps = 1000, type = "bootstrap") %>% 
calculate(stat = "mean")

#bdis is the mean of 1000 bootstrap samples created from values in Weight
# create a 92% cootstrap percentile CI using the values stored in bdis and interpret your confidence interval
PCI <- bdis %>%
  summarize(l = quantile(stat, .04), u=quantile(stat, .96))
PCI

# We are 92% confident that the avg weights for babies carried full term to women 25-29, falls in the above interval.
## ------------------------------------------------------------------------

set.seed(39)
#bootstrapping weight vs. smoker 1000 times from full term and calculating diff of the mean for both vars.
dd <- fullterm %>% 
  specify(Weight ~ Smoker) %>% 
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "diff in means", order = c("No", "Yes"))
#visualize the bootstrapped data
visualize(dd)
#Gets the confidence interval with a level of 90% on the bootstrapped data.
CI <- conf_int(dd, level = 0.90)
CI

# Bootstrap of the diff in means for Weight vs Smoker
# The code is generating 1000 bootstrap samples of the weight of children born to smoking mothers and 1000 samples of children born to non smoking mothers. The bootstrap means between those mothers is stored in stat in the dd object.  Conf_int is used to find a 90% confidence interval for this stat in dd we find that the mean birth weight of children to non smoking mothers is between the values above.

## ------------------------------------------------------------------------
set.seed(19)
null <- fullterm %>% 
  specify(Weight ~ Smoker) %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "diff in means", order = c("No", "Yes"))
pval <- p_value(null, obs_stat = 204, direction = "right")
pval


ftag <- TXBirths2004 %>% 
  filter(Gestation >= 38, Gestation <= 42, Multiple == "No", 
         MothersAge != "under 15", MothersAge != "40-44") %>% 
  droplevels()
mod.aov <- lm(Weight ~ MothersAge, data = ftag)
anova(mod.aov)

## ------------------------------------------------------------------------
set.seed(9)
pF <- ftag %>% 
  specify(Weight ~ MothersAge) %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "F")
pval <- get_pvalue(pF, 6.7592, direction = "right")
pval

# The code is creating a permmutation distribution for the equality of mean weights for 5 different age groups.  That is, it is testing H_0 : mean(1) = mean(2) = ... mean(5) versus atleast 1 mean(i) not equal to mean(j) for some i not equal to j. based on the pvalue of 0 the evidence suggest that the mean weights are not the same.

## ------------------------------------------------------------------------

# 12: Use the information in ftag to test for an association between Smoker and Gender with the chisq.test() function. State your conclusion after running the test.

T1 <- table(ftag$Smoker, ftag$Gender)
T11 <- ftag %>%
  dplyr::select(Smoker, Gender) %>%
  table()
T1
T11
chisq.test(T1)
pval <- chisq.test(T1)$p.value
pval


```
---






```{r}
B <- 10^4
bt <- numeric(B)
for (i in 1:B) {
  bss <- sample(fullterm$Weight, size = sum(!is.na(fullterm$Weight)), replace = TRUE)
  bt[i] <- ((mean(bss)-mean(fullterm$Weight))/(sd(bss)/sqrt(sum(!is.na(fullterm$Weight)))))
}

Q <- quantile(bt, probs = c(0.05, 0.95))
Q

BTCI <- c(mean(fullterm$Weight) - Q[2] * sd(fullterm$Weight)/sqrt(length(fullterm$Weight)),mean(fullterm$Weight) - Q[1] * sd(fullterm$Weight)/sqrt(length(fullterm$Weight)))


smokers <- fullterm %>%
  filter(Smoker == "Yes")

nonsmokers <- fullterm %>%
  filter(Smoker == "No")


B <- 10^4
bt1 <- numeric(B)
for (i in 1:B) {
  bss1 <- sample(nonsmokers$Weight, size = sum(!is.na(nonsmokers$Weight)), replace = TRUE)
  bss2 <- sample(smokers$Weight, size = sum(!is.na(smokers$Weight)), replace = TRUE)
  bt1[i] <- (((mean(bss1) - mean(bss2))-(mean(smokers$Weight)) - mean(nonsmokers$Weight))/sqrt(((sd(bss1)^2)/length(!is.na(smokers$Weight)))+((sd(bss2)^2)/length(!is.na(nonsmokers$Weight)))))
}

perc <- .93 #must be between 1 and 0, 1 = 100%, .9 = 90% ...
l <- (1-(perc))/2
u <- 1-(1-(perc))/2
Q <- quantile(bt1, probs = c(l, u))
Q

t <- qt(u, 22.821)
t

t.test(Weight~Smoker, data = fullterm, conf = perc)


BTCI <- c(mean(fullterm$Weight) - Q[2] * sd(fullterm$Weight)/sqrt(length(fullterm$Weight)),mean(fullterm$Weight) - Q[1] * sd(fullterm$Weight)/sqrt(length(fullterm$Weight)))
BTCI


ft <- fullterm %>%
  group_by(Smoker) %>%
  summarise(Mean = mean(Weight), SD = sd(Weight), n = n())
ft

weightSM <- subset(fullterm, select = Weight, Smoker == "Yes", drop = TRUE)
weightSM

weightNSM <- fullterm %>%
  filter(Smoker == "No")
weightNSM <- weightNSM$Weight
weightNSM

B1 <- 10^4
bt2 <- numeric(B1)

for(i in 1:B1) {
  bss3 <- sample(weightNSM,size = length(weightNSM), replace = TRUE)
  bss4 <- sample(weightSM,size = length(weightSM), replace = TRUE)
  bt2[i] <- (((mean(bss3) - mean(bss4))-(mean(weightSM)) - mean(weightNSM))/sqrt(((sd(bss3)^2)/length(!is.na(weightSM)))+((sd(bss4)^2)/length(!is.na(weightNSM)))))
}


wnsm <- fullterm %>%
  filter(Smoker == "No") %>%
  dplyr::select(Weight) %>%
  pull()


wsm <- fullterm %>%
  filter(Smoker == "Yes") %>%
  dplyr::select(Weight) %>%
  pull()

xbarwnsm <- mean(wnsm)
xbarwsm <- mean(wsm)
theta <- xbarwnsm - xbarwsm

B <- 10^4
bt3 <- numeric(B)
for (i in 1:B){
  bss5 <- sample(wnsm, size = length(wnsm), replace = TRUE)
  bss6 <- sample(wsm, size = length(wsm), replace = TRUE)
  bt3[i] <- ((mean(bss5) - mean(bss6)) - theta)/(sqrt(var(bss5)/length(bss5)+var(bss6)/length(bss6)))
}

Q <- quantile(bt3, probs = c(0.025, .975))

CI <- c(theta - Q[2]*(sqrt(var(wnsm)/length(wnsm)+var(wsm)/length(wsm))),theta - Q[1]*(sqrt(var(wnsm)/length(wnsm)+var(wsm)/length(wsm))))
CI

CI <- theta - Q[2:1]*(sqrt(var(wnsm)/length(wnsm)+var(wsm)/length(wsm)))
CI

#permutation test and pvalue with for loop = test hypothesis that mean weight for non smokers - mean weight for smokers = 0 vs alt that they are not equal to 0

xbarwnsm <- mean(wnsm)
xbarwsm <- mean(wsm)
obs_stat <- xbarwnsm - xbarwsm

pt <- numeric(B)

for(i in 1:B) {
  obs <- tapply(fullterm$Weight, sample(fullterm$Smoker), mean)
  pt[i] <- obs[1] - obs[2]
}

pvalue <- (sum(pt >= obs_stat)*2 + 1)/(B+1)
pvalue

#prob that (1 <= chisq(15) <= 10)
pchisq(10, 15) - pchisq(1, 15)

#prob that (-2 <= tdist(5) <= a) = 0.7 a = ___
(qt(pt(-2, 5)+.7, 5))

#prob that (-a <= Z <= a) = 0.95 => a = ____
res <- .95
val <- (1+res)/2
qnorm(val)


#################################################################

B <- 10^4
bt4 <- numeric(B)
for (i in 1:B){
  bss5 <- sample(wnsm, size = length(wnsm), replace = TRUE)
  bss6 <- sample(wsm, size = length(wsm), replace = TRUE)
  bt4[i] <- ((mean(bss5) / mean(bss6)))
}


perc <- .92 #must be between 1 and 0, 1 = 100%, .9 = 90% ...
l <- (1-(perc))/2
u <- 1-(1-(perc))/2
Q <- quantile(bt4, probs = c(l, u))
Q

t.test(Weight~Smoker, data = fullterm, conf = perc)


BTCI <- c(mean(fullterm$Weight) - Q[2] * sd(fullterm$Weight)/sqrt(length(fullterm$Weight)),mean(fullterm$Weight) - Q[1] * sd(fullterm$Weight)/sqrt(length(fullterm$Weight)))
BTCI

```
---

#Basics
  
##Fundamental Question of Inference
>How does what we actually observe compare to the null hypothesis if we repeat the process many times?
  
##What is a p-value
>The probability that what we observe is greater than or equal to the observation we made if we assume the null hypothesis is true.
  
>>A small p-value (typically ≤ 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis.
  
>>A large p-value (> 0.05) indicates weak evidence against the null hypothesis, so you fail to reject the null hypothesis.
  
>>>If you reject the null hypothesis then you can only have a type I error.
  
>>>If you fail to reject the null hypothesis then you can only have a Type II error.
  
>>>>If they reject or fail or reject and their hypothesis is incorrect they have made an error.
  
  
#Bootstrapping
  
##CLT (Central Limit Theorm)

$x, x_1, x_2, x_3, ... x_n \sim (\mu, \theta)$
    
#Bootstrapping
>The bootstrap is a procedure that uses the given sample to create a new distribution, called the bootstrap distribution, that approximates the sampling distribution for the sample mean (or for other statistics).

---

FIRST CHEAT SHEET

### Verizon Problem

```{r label = "VerizonSetup", echo=FALSE,message = FALSE}
library(ggplot2)
library(dplyr)
url <- "https://raw.githubusercontent.com/alanarnholt/STT3850/gh-pages/DataCSV/Verizon.csv"
Verizon <- read.csv(url)
```
```{r label="Verizon"}
head(Verizon)
ILEC <- Verizon %>% filter(Group == "ILEC")
CLEC <- Verizon %>% filter(Group == "CLEC")
hist(ILEC$Time)
hist(CLEC$Time)
Verizon %>%
ggplot(aes(x = Time,y = ..density..)) +
  geom_histogram(color = "black", fill = "navy") +
  facet_grid(.~Group) +
  theme_bw()

Verizon %>%
ggplot(aes(x = Time)) +
  geom_histogram(color = "black", fill = "navy") +
  facet_grid(.~Group) +
  theme_bw()

Verizon %>%
  group_by(Group) %>%
  summarize(m = n(), mean = mean(Time), SD = sd(Time))

Times <- Verizon$Time
sims <- 10^2 - 1
ans <- numeric(sims)
for (i in 1:sims) {
  index <- sample(1687, 23, FALSE)
  ans[i] <- mean(Times[index]) - mean(Times[-index])
}

OBS <- tapply(Verizon$Time, Verizon$Group, mean)
OBS
OBS_DIFF <- OBS[1] - OBS[2]
OBS_DIFF

pval <- (sum(ans >= OBS_DIFF) + 1)/(sims + 1)
pval


median_ans <- numeric(sims)
for (i in 1:sims) {
  index <- sample(1687, 23, FALSE)
  median_ans[i] <- median(Times[index]) - median(Times[-index])
}

OBS <- tapply(Verizon$Time, Verizon$Group, median)
OBS
OBS_DIFF <- OBS[1] - OBS[2]
OBS_DIFF

pval <- (sum(median_ans >= OBS_DIFF) + 1)/(sims + 1)
pval

trimmed_ans <- numeric(sims)
trimamnt <- 0.3
for (i in 1:sims) {
  index <- sample(1687, 23, FALSE)
  trimmed_ans[i] <- mean(Times[index], trim=trimamnt) - mean(Times[-index], trim=trimamnt)
}

OBS <- tapply(Verizon$Time, Verizon$Group, mean, trim = trimamnt)
OBS
OBS_DIFF <- OBS[1] - OBS[2]
OBS_DIFF

pval <- (sum(trimmed_ans >= OBS_DIFF) + 1)/(sims + 1)
pval
```
---
### Discrimination Assignment

```{r label="DiscriminationSetup"}
library(readxl)
library(dplyr)
library(ggplot2)
DF <- read_excel("TMP.xlsx")
```
```{r label="Discrimination"}
head(DF)

DF$Age_Cohort <- gsub(42898, "6-12", DF$Age_Cohort)
DF$Age_Cohort <- gsub("0 - 5", "0-5", DF$Age_Cohort)
DF$Age_Cohort <- factor(DF$Age_Cohort, levels = c("0-5","6-12","13-17","18-21","22-50","51 +"))
table(DF$Age_Cohort)

DT::datatable(DF)

DF %>% 
  group_by(Gender) %>% 
  summarize(ME = mean(Expenditures), MDE = median(Expenditures), n= n(), SD = sd(Expenditures))

ggplot(data = DF, aes(x = Gender, y = Expenditures, fill = Gender)) + 
  geom_boxplot() + 
  theme_bw() + 
  scale_fill_manual(values = c("pink", "blue"))

DF %>% 
  group_by(Gender) %>% 
  summarize(ME = mean(Expenditures), MDE = median(Expenditures), n= n()) %>%
  ggplot(aes(x = Gender, y= ME, fill = Gender)) + 
  geom_bar(stat = "identity") + 
  labs(title = "Average Expenditure by Gender", y = "Mean Expenditure") + 
  theme_bw() + 
  scale_fill_manual(values = c("pink", "blue"))

DF %>% 
  group_by(Ethnicity) %>% 
  summarize(ME = mean(Expenditures), MDE = median(Expenditures), n= n())

DF %>% 
  group_by(Age_Cohort) %>% 
  summarize(ME = mean(Expenditures), MDE = median(Expenditures), n= n())

DF %>% 
  group_by(Ethnicity) %>% 
  summarize(ME = mean(Expenditures), MDE = median(Expenditures), n= n()) %>%  
  ggplot(aes(x = reorder(Ethnicity, ME), y = ME)) +
  geom_bar(stat="identity", fill = "red") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 50, hjust = 1)) + 
  labs(x = "", y = "Mean Expenditure", title = "Average Expenditure by Ethnicity")

ggplot(data = DF, aes(x = reorder(Ethnicity, Expenditures, median), y = Expenditures)) + 
  geom_boxplot() +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 50, hjust = 1)) +
  labs(x = "")

DF %>% 
  group_by(Age_Cohort) %>% 
  summarize(ME = mean(Expenditures), MDE = median(Expenditures), n= n())

ggplot(data = DF, aes(x = reorder(Age_Cohort, Expenditures, median), y = Expenditures)) + geom_boxplot() + theme_bw()

ggplot(data = DF, aes(x = reorder(Age_Cohort, Expenditures, median), y = Expenditures)) + geom_boxplot() + theme_bw() + 
  facet_grid(.~Gender)

ggplot(data = DF, aes(x = reorder(Age_Cohort, Expenditures, median), y = Expenditures)) + geom_boxplot(varwidth = TRUE) + theme_bw() + 
  facet_grid(Ethnicity ~ Gender)

DF %>% 
  group_by(Gender, Ethnicity) %>% 
  summarize(ME = mean(Expenditures), MDE = median(Expenditures), n= n())

DF %>% 
  group_by(Ethnicity, Age_Cohort) %>% 
  summarize(ME = mean(Expenditures), MDE = median(Expenditures), n= n())

DF %>%
  filter(Ethnicity %in% c("Hispanic", "White not Hispanic")) %>% 
  group_by(Ethnicity) %>% 
  summarize(ME = mean(Expenditures), n = n())

DF %>%
  filter(Ethnicity %in% c("Hispanic", "White not Hispanic")) %>% 
  group_by(Ethnicity) %>% 
  summarize(ME = mean(Expenditures), n = n()) %>% 
  ggplot(aes(x = Ethnicity, y = ME, fill = Ethnicity)) + 
  geom_bar(stat = "identity") +
  theme_bw() + 
  scale_fill_manual(values = c("chocolate", "peachpuff")) + 
  labs(y = "Mean Expenditure")

DF %>%
  filter(Ethnicity %in% c("Hispanic", "White not Hispanic")) %>%
  ggplot(aes(x = Ethnicity, y = Expenditures, fill = Ethnicity)) + 
  geom_boxplot() + 
  theme_bw() + 
  scale_fill_manual(values = c("chocolate", "peachpuff"))

DF %>%
  filter(Ethnicity %in% c("Hispanic", "White not Hispanic")) %>% 
  group_by(Age_Cohort, Ethnicity) %>% 
  summarize(ME = mean(Expenditures), n = n())

DF %>%
  filter(Ethnicity %in% c("Hispanic", "White not Hispanic")) %>% 
  group_by(Age_Cohort, Ethnicity) %>% 
  summarize(ME = mean(Expenditures), n = n()) %>%
  ggplot(aes(x = reorder(Age_Cohort, ME), y = ME, fill = Ethnicity)) + 
  geom_bar(stat = "identity", position = "dodge") + 
  theme_bw() + 
  labs(x = "Age Cohort", y = "Average Expenditure", 
       title = "Average Expenditures by Age Cohort and Ethnicity") + 
  scale_fill_manual(values = c("chocolate", "peachpuff"))

DF %>%
  filter(Ethnicity %in% c("Hispanic", "White not Hispanic")) %>%
  ggplot(aes(x = reorder(Age_Cohort, Expenditures, median), y = Expenditures, fill = Ethnicity)) + 
  geom_boxplot(position = position_dodge(0.9)) + 
  theme_bw() + 
  scale_fill_manual(values = c("chocolate", "peachpuff"))

DF %>%
  filter(Ethnicity %in% c("Hispanic", "White not Hispanic")) %>% 
  group_by(Age_Cohort, Ethnicity) %>% 
  summarize(ME = mean(Expenditures), n = n())

DF %>%
  filter(Ethnicity %in% c("Hispanic", "White not Hispanic")) %>% 
  group_by(Age_Cohort, Ethnicity) %>% 
  summarize(ME = mean(Expenditures), n = n()) %>%
  ggplot(aes(x = reorder(Age_Cohort, ME), y = n, fill = Ethnicity)) + 
  geom_bar(stat = "identity", position = "dodge") + 
  theme_bw() + 
  scale_fill_manual(values = c("chocolate", "peachpuff")) + 
  labs(x = "Age Cohort", y = "Number in Group", title = "Consumers by Ethnicity and Age Cohort")

ggplot(data = subset(DF,Ethnicity == "White not Hispanic" | Ethnicity == "Hispanic" ), aes(x = Age, y = Expenditures, color = Ethnicity)) + 
  geom_point(alpha = 0.8) +
  geom_smooth(se = FALSE) + 
  theme_bw() + 
  scale_color_manual(values = c("chocolate", "peachpuff"))

##Tests?

NDF <- DF %>%
  filter(Ethnicity %in% c("Hispanic", "White not Hispanic")) %>% 
  dplyr::select(Ethnicity, Expenditures)
NDF %>% 
  group_by(Ethnicity) %>% 
  summarize(AVG = mean(Expenditures))

DT::datatable(NDF)

White <- NDF$Expenditures[NDF$Ethnicity=="White not Hispanic"]
mean(White)

Hispanic <- NDF$Expenditures[NDF$Ethnicity=="Hispanic"]
mean(Hispanic)

nW <- length(White)
nH <- length(Hispanic)
BV <- c(White, Hispanic)
obs_diff <- mean(White) - mean(Hispanic)
obs_diff

sims <- 10^4 - 1
ans <- numeric(sims)
for(i in 1:sims){
  index <- sample(nW + nH, nW)
  ans[i] <- mean(BV[index]) - mean(BV[-index])
}
hist(ans, xlim = c(-obs_diff-10, obs_diff+10))
abline(v = obs_diff)

ggplot(data = data.frame(x = ans), aes(x = x)) + 
  geom_histogram(binwidth = 500, fill = "pink", color = "black") + 
  theme_bw() + 
  geom_vline(xintercept = obs_diff)

pvalue <- (sum(ans >= obs_diff) + 1)/(sims + 1)
pvalue

model <- lm(Expenditures ~ Age_Cohort * Ethnicity, 
            data = subset(DF, Ethnicity == "White not Hispanic" | Ethnicity == "Hispanic"))
summary(model)
```

<center><h1>Exam 2 Toolbox</h1></center>
  
  #Basics
  
  ##Fundamental Question of Inference
  >How does what we actually observe compare to the null hypothesis if we repeat the process many times?
  
  ##What is a p-value
  >The probability that what we observe is greater than or equal to the observation we made if we assume the null hypothesis is true.
  
  >>A small p-value (typically ≤ 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis.
  
  >>A large p-value (> 0.05) indicates weak evidence against the null hypothesis, so you fail to reject the null hypothesis.
  
  
  #Bootstrapping
  
  ##CLT (Central Limit Theorm)
    $x, x_1, x_2, x_3, ... x_n \sim (\mu, \theta)$
    
  ##Bootstrapping
  >The bootstrap is a procedure that uses the given sample to create a new distribution, called the bootstrap distribution, that approximates the sampling distribution for the sample mean (or for other statistics).

Consider the bootstrapped data in \@ref(fig:rent_plot)

```{r}
reps <-150
manhattan <- read.csv("https://assets.datacamp.com/production/course_5103/datasets/manhattan.csv")
# Generate bootstrap distribution of medians
rent_ci_med <- manhattan %>%
  # Specify the variable of interest
  specify(response = rent) %>%  
  # Generate 15000 bootstrap samples
  generate(reps = reps, type = "bootstrap") %>% 
  # Calculate the median of each bootstrap sample
  calculate(stat = "median")
```

```{r rent_plot, fig.cap="histogram of rent_ci_med"}
# Plot a histogram of rent_ci_med
#{ggplot(rent_ci_med, aes(stat)) +
  #geom_histogram(binwidth = 50)}
hist(rent_ci_med$stat, col="darkgrey") +
  theme_bw()
```

```{r}
# Percentile method
rent_ci_med %>%
  summarize(l = quantile(stat, 0.025),
            u = quantile(stat, 0.975))
# Standard error method
# Calculate observed median
rent_med_obs <- manhattan %>%
  # Calculate observed median rent
  summarize(median(rent)) %>%
  # Extract numerical value
  pull()                      
# Determine critical value
t_star <- qt(0.975, df = nrow(manhattan) - 1)
# Construct interval
rent_ci_med %>%
  summarize(boot_se = sd(stat)) %>%
  summarize(l = rent_med_obs - t_star * boot_se,
            u = rent_med_obs + t_star * boot_se)
sims <- reps
MED <- numeric(sims)
for(i in 1:sims) {
  bss <- sample(manhattan$rent, 20, replace = TRUE)
  MED[i] <- median(bss)
}
CI_perc <- quantile(MED, probs = c(.025, .975))
CI_perc
# Remove NA visits
#ncbirths_complete_visits <- !is.na(ncbirths$visits)
# Generate #reps bootstrap means
#visit_ci_mean <- ncbirths_complete_visits %>%
#  specify(response = visits) %>%
#  generate(reps = reps, type = "bootstrap") %>%
#  calculate(stat = "mean")
# Calculate the 90% CI via percentile method
#visit_ci_mean %>%
#  summarize(l = quantile(stat, 0.05),
#            u = quantile(stat, 0.95))
# Calculate #reps bootstrap SDs
#visit_ci_sd <- ncbirths_complete_visits %>%
#  specify(response = visits) %>%
#  generate(reps = reps, type = "bootstrap") %>%
#  calculate(stat = "sd")
  
# Calculate the 90% CI via percentile method
#visit_ci_sd %>%
#  summarize(l = quantile(stat, 0.05),
#            u = quantile(stat, 0.95))
# Generate `reps` bootstrap samples centered at null
#rent_med_ht <- manhattan %>%
#  specify(response = rent) %>%
#  hypothesize(null = "point", med = 2500) %>% 
#  generate(reps = reps, type = "bootstrap") %>% 
#  calculate(stat = "median")
# Calculate observed median
#rent_med_obs <- manhattan %>%
#  summarize(median(rent)) %>%
#  pull()
# Calculate p-value
#rent_ht_pval <- rent_med_ht %>%
#  filter(stat > rent_med_obs) %>%
#  summarize(n() / reps)

#The p-value for rent_med_ht is: `r rent_ht_pval`

```

```{r}

qnorm(c(.1, .25, .75, .9), 90, 5)

```

---
View the null distrubution of a bootstrapping of weight vs smokers in \@ref(fig:weightvssmoke)
```{r}
## Pre test Code 

library(infer)
library(resampledata)
library(tidyverse)
#fullterm data is TXBirths where the age is 25-29 gestation is greater than 38 but less or equal to than 42 and no multiples.
fullterm <- TXBirths2004 %>% 
  filter(MothersAge == "25-29", Gestation >= 38, 
         Gestation <= 42, Multiple == "No")

## ------------------------------------------------------------------------
set.seed(49)
library(infer)
#bootstrapping weight 1000 times from full term and calculating the mean.
bdis <- fullterm %>% 
specify(response = Weight) %>% 
generate(reps = 1000, type = "bootstrap") %>% 
calculate(stat = "mean")

```

```{r weightvssmoke}
set.seed(39)
#bootstrapping weight vs. smoker 1000 times from full term and calculating diff of the mean for both vars.
dd <- fullterm %>% 
  specify(Weight ~ Smoker) %>% 
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "diff in means", order = c("No", "Yes"))
#visualize the bootstrapped data
visualize(dd)
#Gets the confidence interval with a level of 90% on the bootstrapped data.
CI <- conf_int(dd, level = 0.90)
CI
```

```{r}
set.seed(19)
null <- fullterm %>% 
  specify(Weight ~ Smoker) %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "diff in means", order = c("No", "Yes"))
pval <- p_value(null, obs_stat = 196.924, direction = "right")
pval


ftag <- TXBirths2004 %>% 
  filter(Gestation >= 38, Gestation <= 42, Multiple == "No", 
         MothersAge != "under 15", MothersAge != "40-44") %>% 
  droplevels()

#Fitting Linear Models
#Description
#lm is used to fit linear models. It can be used to carry out regression, single stratum analysis of variance and analysis of covariance (although aov may provide a more convenient interface for these).
#Usage
#lm(formula, data, subset, weights, na.action,
#   method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,
#   singular.ok = TRUE, contrasts = NULL, offset, ...)
#Arguments
#formula	
#an object of class "formula" (or one that can be coerced to that class): a symbolic description of the model to be fitted. The details of model specification are given under ‘Details’.
#data	
#an optional data frame, list or environment (or object coercible by as.data.frame to a data frame) containing the variables in the model. If not found in data, the variables are taken from environment(formula), typically the environment from which lm is called.
#subset	
#an optional vector specifying a subset of observations to be used in the fitting process.
#weights	
#an optional vector of weights to be used in the fitting process. Should be NULL or a numeric vector. If non-NULL, weighted least squares is used with weights weights (that is, minimizing sum(w*e^2)); otherwise ordinary least squares is used. See also ‘Details’,
#na.action	
#a function which indicates what should happen when the data contain NAs. The default is set by the na.action setting of options, and is na.fail if that is unset. The ‘factory-fresh’ default is na.omit. Another possible value is NULL, no action. Value na.exclude can be useful.
#method	
#the method to be used; for fitting, currently only method = "qr" is supported; method = "model.frame" returns the model frame (the same as with model = TRUE, see below).
#model, x, y, qr	
#logicals. If TRUE the corresponding components of the fit (the model frame, the model matrix, the response, the QR decomposition) are returned.
#singular.ok	
#logical. If FALSE (the default in S but not in R) a singular fit is an error.
#contrasts	
#an optional list. See the contrasts.arg of model.matrix.default.
#offset	
#this can be used to specify an a priori known component to be included in the linear predictor during fitting. This should be NULL or a numeric vector of length equal to the number of cases. One or more offset terms can be included in the formula instead or as well, and if more than one are specified their sum is used. See model.offset.
#...	
#additional arguments to be passed to the low level regression fitting functions (see below).
#Details
#Models for lm are specified symbolically. A typical model has the form response ~ terms where response is the (numeric) response vector and terms is a series of terms which specifies a linear predictor for response. A terms specification of the form first + second indicates all the terms in first together with all the terms in second with duplicates removed. A specification of the form first:second indicates the set of terms obtained by taking the interactions of all terms in first with all terms in second. The specification first*second indicates the cross of first and second. This is the same as first + second + first:second.
#If the formula includes an offset, this is evaluated and subtracted from the response.
#If response is a matrix a linear model is fitted separately by least-squares to each column of the matrix.
#See model.matrix for some further details. The terms in the formula will be re-ordered so that main effects come first, followed by the interactions, all second-order, all third-order and so on: to avoid this pass a terms object as the formula (see aov and demo(glm.vr) for an example).
#A formula has an implied intercept term. To remove this use either y ~ x - 1 or y ~ 0 + x. See formula for more details of allowed formulae.
#Non-NULL weights can be used to indicate that different observations have different variances (with the values in weights being inversely proportional to the variances); or equivalently, when the elements of weights are positive integers w_i, that each response y_i is the mean of w_i unit-weight observations (including the case that there are w_i observations equal to y_i and the data have been summarized). However, in the latter case, notice that within-group variation is not used. Therefore, the sigma estimate and residual degrees of freedom may be suboptimal; in the case of replication weights, even wrong. Hence, standard errors and analysis of variance tables should be treated with care.
#lm calls the lower level functions lm.fit, etc, see below, for the actual numerical computations. For programming only, you may consider doing likewise.
#All of weights, subset and offset are evaluated in the same way as variables in formula, that is first in data and then in the environment of formula.
#Value
#lm returns an object of class "lm" or for multiple responses of class c("mlm", "lm").
#The functions summary and anova are used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm.
#An object of class "lm" is a list containing at least the following components:
#coefficients	
#a named vector of coefficients
#residuals	
#the residuals, that is response minus fitted values.
#fitted.values	
#the fitted mean values.
#rank	
#the numeric rank of the fitted linear model.
#weights	
#(only for weighted fits) the specified weights.
#df.residual	
#the residual degrees of freedom.
#call	
#the matched call.
#terms	
#the terms object used.
#contrasts	
#(only where relevant) the contrasts used.
#xlevels	
#(only where relevant) a record of the levels of the factors used in fitting.
#offset	
#the offset used (missing if none were used).
#y	
#if requested, the response used.
#x	
#if requested, the model matrix used.
#model	
#if requested (the default), the model frame used.
#na.action	
#(where relevant) information returned by model.frame on the special handling of NAs.
#In addition, non-null fits will have components assign, effects and (unless not requested) qr relating to the linear fit, for use by extractor functions such as summary and effects.

mod.aov <- lm(Weight ~ MothersAge, data = ftag)
#Anova Tables
#Description
#Compute analysis of variance (or deviance) tables for one or more fitted model objects.
#Usage
#anova(object, ...)
#Arguments
#object	
#an object containing the results returned by a model fitting function (e.g., lm or glm).
#...	
#additional objects of the same type.
#Value
#This (generic) function returns an object of class anova. These objects represent analysis-of-variance and analysis-of-deviance tables. When given a single argument it produces a table which tests whether the model terms are significant.
#When given a sequence of objects, anova tests the models against one another in the order specified.
#The print method for anova objects prints tables in a ‘pretty’ form.
#Warning
#The comparison between two or more models will only be valid if they are fitted to the same dataset. This may be a problem if there are missing values and R's default of na.action = na.omit is used.
anova(mod.aov)

set.seed(9)
pF <- ftag %>% 
  specify(Weight ~ MothersAge) %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "F")
#F Distribution
#If V 1 and V 2 are two independent random variables having the Chi-Squared distribution with m1 and m2 degrees of freedom respectively, then the following quantity follows an F distribution with m1 numerator degrees of freedom and m2 denominator degrees of freedom, i.e., (m1,m2) degrees of freedom.
pval <- get_pvalue(pF, 6.7592, direction = "right")
pval
```

